ğŸŒŸ Sometimes the best ideas come when you least expect themâ€¦
Iâ€™d been down with a fever for the past few days, not feeling well at all. But in the middle of resting, a random idea popped into my mind:
 â€œWhy not play with LLMs while Iâ€™m stuck at home?â€
So I decided to turn this downtime into learning time â€” and thatâ€™s how I ended up building my own local AI chatbot using:
 âœ… FastAPI
 âœ… Ollama + Gemma3
 âœ… Gradio
ğŸ¤– What I built
A simple, clean chatbot that:
Uses FastAPI for the backend
Runs Gemma3 locally via Ollama
Streams responses live with FastAPIâ€™s StreamingResponse
Has a Gradio UI for chatting in real time
Keeps session history so the conversation feels natural
All in one Python file â€” no cloud, no extra servers, no cost.
 Just run:
bash:
uvicorn chatbot_api:app --reload
â€¦and chat with your LLM at http://127.0.0.1:8000/gradio.
ğŸ’¡ Why this is exciting
I love how accessible local LLMs are becoming â€” you donâ€™t need a huge setup to experiment and learn. Even when youâ€™re not at your best, you can still build something meaningful.
This small project reminded me:
Learning never stops â€” even when youâ€™re under the weather.
If anyoneâ€™s curious about the full code or wants help setting up their own local LLM playground, Iâ€™m happy to share! Just comment â€œğŸ”—â€ below.
ğŸ‘‡ Have you ever built or tested an LLM on your own machine?
 Would love to hear your experiences!
hashtag#LLM hashtag#FastAPI hashtag#Gemma3 hashtag#Ollama hashtag#Gradio hashtag#Python hashtag#AI hashtag#Streaming hashtag#LearningByDoing hashtag#OpenSource
